# ë©´ì ‘ ì§ˆë¬¸ ìƒì„± API ì„œë¹„ìŠ¤

## ê°œìš”
ì´ë ¥ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œì¸í™”ëœ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” LangGraph API ì„œë¹„ìŠ¤. Dolphinìœ¼ë¡œ ì´ë ¥ì„œë¥¼ íŒŒì‹±í•˜ê³ , ì´ì „ ì§ˆë¬¸ ì´ë ¥ì„ ê³ ë ¤í•˜ì—¬ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ë§ì¶¤í˜• ì§ˆë¬¸ì„ ìƒì„±. ìŠ¤ì¼€ì¤„ë§ ë°œì†¡ì€ ì™¸ë¶€ ì„œë²„ì—ì„œ ë‹´ë‹¹í•˜ë©°, ì´ ì„œë¹„ìŠ¤ëŠ” ìˆœìˆ˜ ì§ˆë¬¸ ìƒì„± ë¡œì§ë§Œ ì œê³µ.

## 1. ì„œë¹„ìŠ¤ ëª¨ë¸ ì„¤ê³„

### 1.1 API ì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜

```mermaid
graph LR
    External[ì™¸ë¶€ ìŠ¤ì¼€ì¤„ë§ ì„œë²„] -->|API í˜¸ì¶œ| QuestionAPI[Question Generation API]

    subgraph "Question Generation Service"
        QuestionAPI --> LangGraph[LangGraph Workflow]

        subgraph "LangGraph State Machine"
            LoadProfile[í”„ë¡œí•„ ë¡œë“œ]
            CheckHistory[ì§ˆë¬¸ ì´ë ¥ í™•ì¸]
            GenerateQ[ì§ˆë¬¸ ìƒì„±]
            ValidateQ[ì¤‘ë³µ ê²€ì¦]
            SaveHistory[ì´ë ¥ ì €ì¥]
        end

        LangGraph --> Response[JSON Response]
    end

    Response --> External
```

### 1.2 ë‹¤ì¤‘ ì´ë ¥ì„œ ì§€ì› API ì„¤ê³„

```python
from fastapi import FastAPI, HTTPException, UploadFile, File
from pydantic import BaseModel
from typing import List, Optional
import uuid

app = FastAPI(title="Interview Question Generator")

class QuestionRequest(BaseModel):
    user_id: str
    resume_id: Optional[str] = None  # íŠ¹ì • ì´ë ¥ì„œ ì§€ì •
    session_id: Optional[str] = None  # ì§ˆë¬¸ ì„¸ì…˜ ID
    target_position: str
    difficulty_preference: float = 0.5
    excluded_categories: List[str] = []

class QuestionResponse(BaseModel):
    question_id: str
    question: str
    category: str
    difficulty: float
    estimated_time: int
    context: dict
    learning_objectives: List[str]
    resume_id: str  # ì–´ë–¤ ì´ë ¥ì„œ ê¸°ë°˜ì¸ì§€
    session_id: Optional[str]

class ResumeInfo(BaseModel):
    resume_id: str
    file_name: str
    is_primary: bool
    parsing_confidence: float
    created_at: str

# 1. ì´ë ¥ì„œ ê´€ë¦¬ API
@app.post("/user/{user_id}/resumes")
async def upload_resume(
    user_id: str,
    file: UploadFile = File(...),
    is_primary: bool = False
):
    """ì´ë ¥ì„œ ì—…ë¡œë“œ ë° íŒŒì‹±"""

    # íŒŒì¼ ì €ì¥
    file_path = await save_uploaded_file(file)

    # Dolphin íŒŒì‹±
    parsed_data = await parse_resume_with_dolphin(file_path)

    # DB ì €ì¥
    resume_id = str(uuid.uuid4())
    await save_resume({
        "resume_id": resume_id,
        "user_id": user_id,
        "file_name": file.filename,
        "file_path": file_path,
        "parsed_data": parsed_data,
        "parsing_confidence": parsed_data["metadata"]["confidence_score"],
        "is_primary": is_primary
    })

    return {"resume_id": resume_id, "status": "uploaded"}

@app.get("/user/{user_id}/resumes", response_model=List[ResumeInfo])
async def get_user_resumes(user_id: str):
    """ì‚¬ìš©ìì˜ ëª¨ë“  ì´ë ¥ì„œ ì¡°íšŒ"""
    resumes = await load_user_resumes(user_id)
    return [ResumeInfo(**resume) for resume in resumes]

@app.delete("/user/{user_id}/resumes/{resume_id}")
async def delete_resume(user_id: str, resume_id: str):
    """ì´ë ¥ì„œ ì‚­ì œ"""
    await remove_resume(user_id, resume_id)
    return {"status": "deleted"}

# 2. ì§ˆë¬¸ ì„¸ì…˜ ê´€ë¦¬ API
@app.post("/user/{user_id}/sessions")
async def create_question_session(
    user_id: str,
    resume_id: str,
    target_position: str,
    session_name: str
):
    """ìƒˆ ì§ˆë¬¸ ì„¸ì…˜ ìƒì„±"""
    session_id = str(uuid.uuid4())

    await save_question_session({
        "session_id": session_id,
        "user_id": user_id,
        "resume_id": resume_id,
        "target_position": target_position,
        "session_name": session_name,
        "is_active": True
    })

    return {"session_id": session_id}

@app.get("/user/{user_id}/sessions")
async def get_user_sessions(user_id: str):
    """ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì„¸ì…˜ ëª©ë¡"""
    sessions = await load_user_sessions(user_id)
    return sessions

# 3. ì§ˆë¬¸ ìƒì„± API (ê°œì„ ë¨)
@app.post("/generate-question", response_model=QuestionResponse)
async def generate_question(request: QuestionRequest):
    """ë‹¤ì¤‘ ì´ë ¥ì„œ ì§€ì› ì§ˆë¬¸ ìƒì„±"""

    # ì´ë ¥ì„œ ê²°ì • ë¡œì§
    if request.resume_id:
        resume_id = request.resume_id
    elif request.session_id:
        session = await load_session(request.session_id)
        resume_id = session["resume_id"]
    else:
        # ê¸°ë³¸ ì´ë ¥ì„œ ì‚¬ìš©
        primary_resume = await get_primary_resume(request.user_id)
        resume_id = primary_resume["resume_id"]

    # ì§ˆë¬¸ ì´ë ¥ ë¡œë“œ (ì´ë ¥ì„œë³„)
    question_history = await load_question_history(
        user_id=request.user_id,
        resume_id=resume_id
    )

    workflow = create_question_generation_workflow()

    initial_state = {
        "user_id": request.user_id,
        "resume_id": resume_id,
        "session_id": request.session_id,
        "target_position": request.target_position,
        "difficulty_preference": request.difficulty_preference,
        "excluded_categories": request.excluded_categories,
        "question_history": question_history
    }

    result = await workflow.ainvoke(initial_state)

    if result.get("error"):
        raise HTTPException(status_code=400, detail=result["error"])

    return QuestionResponse(**result["generated_question"])

# 4. ë¶„ì„ API
@app.get("/user/{user_id}/analytics")
async def get_user_analytics(user_id: str, resume_id: Optional[str] = None):
    """ì‚¬ìš©ì ë¶„ì„ (ì „ì²´ ë˜ëŠ” ì´ë ¥ì„œë³„)"""

    if resume_id:
        # íŠ¹ì • ì´ë ¥ì„œ ê¸°ë°˜ ë¶„ì„
        analytics = await calculate_resume_analytics(user_id, resume_id)
    else:
        # ì „ì²´ ì´ë ¥ì„œ í†µí•© ë¶„ì„
        analytics = await calculate_user_analytics(user_id)

    return analytics

@app.get("/user/{user_id}/resumes/{resume_id}/comparison")
async def compare_resumes(user_id: str, resume_id: str, compare_with: str):
    """ì´ë ¥ì„œ ê°„ ë¹„êµ ë¶„ì„"""

    comparison = await compare_resume_profiles(
        user_id=user_id,
        resume_id_1=resume_id,
        resume_id_2=compare_with
    )

    return {
        "skill_differences": comparison["skill_gaps"],
        "experience_differences": comparison["experience_gaps"],
        "question_difficulty_comparison": comparison["difficulty_trends"],
        "recommendation": comparison["switch_recommendation"]
    }
```

### 1.2 í•µì‹¬ ì»´í¬ë„ŒíŠ¸

#### ì§ˆë¬¸ ìƒì„± ì—”ì§„
- **ì´ë ¥ì„œ ë¶„ì„**: ê²½ë ¥/ê¸°ìˆ /í”„ë¡œì íŠ¸ ì¶”ì¶œ
- **ê°œì¸í™” ì•Œê³ ë¦¬ì¦˜**: ì•½ì /ê°•ì  ê¸°ë°˜ ì§ˆë¬¸ ìš°ì„ ìˆœìœ„
- **ë‚œì´ë„ ê³¡ì„ **: ì ì§„ì  ì–´ë ¤ì›€ ì¦ê°€
- **ë‹¤ì–‘ì„± ë³´ì¥**: ê¸°ìˆ /í–‰ë™/ìƒí™© ì§ˆë¬¸ ê· í˜•

#### ìŠ¤ì¼€ì¤„ë§ ì‹œìŠ¤í…œ
- **n8n í¬ë¡ ì¡**: ë§¤ì¼ ì •í•´ì§„ ì‹œê°„ ì‹¤í–‰
- **ì‹œê°„ëŒ€ ëŒ€ì‘**: ì‚¬ìš©ìë³„ ë¡œì»¬ ì‹œê°„
- **íœ´ì¼ ìŠ¤í‚µ**: ì£¼ë§/ê³µíœ´ì¼ ì˜µì…˜
- **ì¬ì‹œë„ ë¡œì§**: ì „ì†¡ ì‹¤íŒ¨ì‹œ ì¬ë°œì†¡

#### í”¼ë“œë°± ì‹œìŠ¤í…œ
- **ì¦‰ì‹œ í”¼ë“œë°±**: ë‹µë³€ ì œì¶œ í›„ 5ë¶„ ë‚´
- **ìƒì„¸ ë¶„ì„**: ê°•ì /ê°œì„ ì /ì¶”ì²œì‚¬í•­
- **ì ìˆ˜ ì¶”ì **: ì§„í–‰ë„ ì‹œê°í™”
- **ê°œì„  ì œì•ˆ**: ë‹¤ìŒ ë‹¨ê³„ ê°€ì´ë“œ

## 2. LangGraph ì›Œí¬í”Œë¡œìš° ì„¤ê³„

### 2.1 ì§ˆë¬¸ ìƒì„± ìƒíƒœ ë¨¸ì‹ 

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, List, Optional

class QuestionGenerationState(TypedDict):
    user_id: str
    resume_id: str  # ì¶”ê°€: ì–´ë–¤ ì´ë ¥ì„œ ê¸°ë°˜ì¸ì§€
    session_id: Optional[str]  # ì¶”ê°€: ì§ˆë¬¸ ì„¸ì…˜ ID
    resume_data: Optional[dict]
    target_position: str
    difficulty_preference: float
    excluded_categories: List[str]
    question_history: List[dict]  # ì´ì œ resume_idë³„ë¡œ í•„í„°ë§ë¨
    user_profile: Optional[dict]
    candidate_topics: List[dict]
    generated_question: Optional[dict]
    error: Optional[str]

def create_question_generation_workflow():
    """ì§ˆë¬¸ ìƒì„± LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±"""

    workflow = StateGraph(QuestionGenerationState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("load_user_profile", load_user_profile_node)
    workflow.add_node("analyze_question_history", analyze_history_node)
    workflow.add_node("generate_candidate_topics", generate_topics_node)
    workflow.add_node("select_best_topic", select_topic_node)
    workflow.add_node("generate_question", generate_question_node)
    workflow.add_node("validate_uniqueness", validate_uniqueness_node)
    workflow.add_node("save_question_history", save_history_node)
    workflow.add_node("handle_error", error_handler_node)

    # ì›Œí¬í”Œë¡œìš° ì—°ê²°
    workflow.set_entry_point("load_user_profile")

    workflow.add_edge("load_user_profile", "analyze_question_history")
    workflow.add_edge("analyze_question_history", "generate_candidate_topics")
    workflow.add_edge("generate_candidate_topics", "select_best_topic")
    workflow.add_edge("select_best_topic", "generate_question")
    workflow.add_edge("generate_question", "validate_uniqueness")

    # ì¡°ê±´ë¶€ ì—£ì§€
    workflow.add_conditional_edges(
        "validate_uniqueness",
        route_validation_result,
        {
            "unique": "save_question_history",
            "duplicate": "generate_candidate_topics",  # ì¬ìƒì„±
            "error": "handle_error"
        }
    )

    workflow.add_edge("save_question_history", END)
    workflow.add_edge("handle_error", END)

    return workflow.compile()

def route_validation_result(state: QuestionGenerationState) -> str:
    """ê²€ì¦ ê²°ê³¼ì— ë”°ë¥¸ ë¼ìš°íŒ…"""
    if state.get("error"):
        return "error"

    question = state.get("generated_question", {})
    if question.get("is_duplicate", False):
        return "duplicate"
    else:
        return "unique"
```

### 2.2 ì¤‘ë³µ ë°©ì§€ ë¡œì§

```python
async def validate_uniqueness_node(state: QuestionGenerationState) -> QuestionGenerationState:
    """ì§ˆë¬¸ ì¤‘ë³µ ê²€ì¦ ë…¸ë“œ"""

    generated_question = state["generated_question"]
    question_history = state["question_history"]

    # 1. í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê²€ì‚¬
    similarity_scores = []
    for past_question in question_history:
        similarity = calculate_text_similarity(
            generated_question["question"],
            past_question["question"]
        )
        similarity_scores.append(similarity)

    max_similarity = max(similarity_scores) if similarity_scores else 0.0

    # 2. ì£¼ì œ/í‚¤ì›Œë“œ ì¤‘ë³µ ê²€ì‚¬
    topic_overlap = check_topic_overlap(
        generated_question["learning_objectives"],
        [q["learning_objectives"] for q in question_history]
    )

    # 3. ì¤‘ë³µ íŒì •
    is_duplicate = (
        max_similarity > 0.8 or  # 80% ì´ìƒ ìœ ì‚¬
        topic_overlap > 0.7      # 70% ì´ìƒ ì£¼ì œ ì¤‘ë³µ
    )

    if is_duplicate:
        # ì¬ìƒì„±ì„ ìœ„í•œ ì œì•½ ì¡°ê±´ ì¶”ê°€
        excluded_topics = extract_excluded_topics(question_history)
        return {
            **state,
            "generated_question": {**generated_question, "is_duplicate": True},
            "excluded_topics": excluded_topics,
            "retry_count": state.get("retry_count", 0) + 1
        }

    return {
        **state,
        "generated_question": {**generated_question, "is_duplicate": False}
    }

def calculate_text_similarity(text1: str, text2: str) -> float:
    """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (ì„ë² ë”© ê¸°ë°˜)"""
    # ì‹¤ì œ êµ¬í˜„ì‹œ sentence-transformers ì‚¬ìš©
    from sentence_transformers import SentenceTransformer

    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode([text1, text2])

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    from sklearn.metrics.pairwise import cosine_similarity
    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]

    return float(similarity)

def check_topic_overlap(new_topics: List[str], past_topics_list: List[List[str]]) -> float:
    """ì£¼ì œ ì¤‘ë³µë„ ê²€ì‚¬"""
    if not past_topics_list:
        return 0.0

    all_past_topics = set()
    for topics in past_topics_list:
        all_past_topics.update(topics)

    new_topics_set = set(new_topics)
    overlap = len(new_topics_set.intersection(all_past_topics))

    return overlap / len(new_topics_set) if new_topics_set else 0.0
```

## 3. ì™„ì „í•œ LangGraph ì§ˆë¬¸ ìƒì„± ë‹¤ì´ì–´ê·¸ë¨

### 3.1 LangGraph State Machine ì‹œê°í™”

```mermaid
graph TB
    Start([API í˜¸ì¶œ<br/>POST /generate-question]) --> LoadProfile[load_user_profile<br/>ì‚¬ìš©ì í”„ë¡œí•„ ë¡œë“œ]

    LoadProfile --> CheckResume{ì´ë ¥ì„œ ë°ì´í„°<br/>ì¡´ì¬ ì—¬ë¶€}
    CheckResume -->|ì—†ìŒ| ParseResume[parse_resume<br/>Dolphin API í˜¸ì¶œ]
    CheckResume -->|ìˆìŒ| AnalyzeHistory[analyze_question_history<br/>ì§ˆë¬¸ ì´ë ¥ ë¶„ì„]

    ParseResume --> CreateProfile[create_user_profile<br/>í”„ë¡œí•„ ìƒì„±]
    CreateProfile --> AnalyzeHistory

    AnalyzeHistory --> GenerateTopics[generate_candidate_topics<br/>í›„ë³´ ì£¼ì œ ìƒì„±]
    GenerateTopics --> SelectTopic[select_best_topic<br/>ìµœì  ì£¼ì œ ì„ íƒ]

    SelectTopic --> GenerateQ[generate_question<br/>ì§ˆë¬¸ ìƒì„± (LLM)]
    GenerateQ --> ValidateUnique{validate_uniqueness<br/>ì¤‘ë³µ ê²€ì¦}

    ValidateUnique -->|ì¤‘ë³µë¨| CheckRetry{ì¬ì‹œë„ íšŸìˆ˜<br/>< 3}
    CheckRetry -->|Yes| GenerateTopics
    CheckRetry -->|No| HandleError[handle_error<br/>ì—ëŸ¬ ì²˜ë¦¬]

    ValidateUnique -->|ê³ ìœ í•¨| SaveHistory[save_question_history<br/>ì§ˆë¬¸ ì´ë ¥ ì €ì¥]
    SaveHistory --> ReturnQuestion[ì§ˆë¬¸ ì‘ë‹µ ë°˜í™˜]

    HandleError --> ReturnError[ì—ëŸ¬ ì‘ë‹µ]
    ReturnQuestion --> End([API ì‘ë‹µ])
    ReturnError --> End

    %% ìŠ¤íƒ€ì¼ë§
    classDef startEnd fill:#e1f5fe
    classDef process fill:#f3e5f5
    classDef decision fill:#fff3e0
    classDef llm fill:#e8f5e8
    classDef storage fill:#fce4ec
    classDef error fill:#ffebee

    class Start,End,ReturnQuestion,ReturnError startEnd
    class LoadProfile,ParseResume,CreateProfile,GenerateTopics,SelectTopic,SaveHistory process
    class CheckResume,ValidateUnique,CheckRetry decision
    class GenerateQ llm
    class AnalyzeHistory storage
    class HandleError error
```

### 3.2 LangGraph ìƒíƒœ ì°¨íŠ¸ (State Chart)

```mermaid
stateDiagram-v2
    [*] --> LoadingProfile : API Request

    state LoadingProfile {
        [*] --> CheckCache : Load User Profile
        CheckCache --> CacheHit : Profile exists
        CheckCache --> CacheMiss : No profile

        CacheHit --> ProfileLoaded
        CacheMiss --> ParseResume

        state ParseResume {
            [*] --> DolphinCall : Call Dolphin API
            DolphinCall --> ParsingSuccess : Success
            DolphinCall --> ParsingFailed : Failed

            ParsingSuccess --> CreateProfile : Extract data
            ParsingFailed --> [*] : Error

            CreateProfile --> ProfileCreated
            ProfileCreated --> [*]
        }

        ParseResume --> ProfileLoaded : Profile created
        ProfileLoaded --> [*]
    }

    LoadingProfile --> AnalyzingHistory : Profile ready
    LoadingProfile --> ErrorHandling : Load failed

    state AnalyzingHistory {
        [*] --> CheckHistory : Get question history
        CheckHistory --> EmptyHistory : No questions
        CheckHistory --> HasHistory : Questions exist

        EmptyHistory --> FirstTimeAnalysis
        HasHistory --> DetailedAnalysis

        state DetailedAnalysis {
            [*] --> CategoryDistribution
            CategoryDistribution --> TopicCoverage
            TopicCoverage --> GapIdentification
            GapIdentification --> TrendAnalysis
            TrendAnalysis --> [*]
        }

        FirstTimeAnalysis --> AnalysisComplete
        DetailedAnalysis --> AnalysisComplete
        AnalysisComplete --> [*]
    }

    AnalyzingHistory --> GeneratingTopics : Analysis complete

    state GeneratingTopics {
        [*] --> WeaknessTopics : Generate weakness-based topics
        WeaknessTopics --> ExperienceTopics : Add experience topics
        ExperienceTopics --> BalanceTopics : Add balance topics
        BalanceTopics --> PrioritizeTopics : Sort by priority
        PrioritizeTopics --> TopicsGenerated
        TopicsGenerated --> [*]
    }

    GeneratingTopics --> SelectingTopic : Topics ready

    state SelectingTopic {
        [*] --> CheckExclusions : Apply exclusions
        CheckExclusions --> SelectBest : Pick highest priority
        SelectBest --> TopicSelected
        TopicSelected --> [*]
    }

    SelectingTopic --> GeneratingQuestion : Topic selected

    state GeneratingQuestion {
        [*] --> BuildPrompt : Create LLM prompt
        BuildPrompt --> CallLLM : Send to Claude/GPT
        CallLLM --> LLMSuccess : Response received
        CallLLM --> LLMFailed : API error

        LLMSuccess --> ParseResponse : Extract JSON
        ParseResponse --> QuestionGenerated
        LLMFailed --> RetryLLM : Retry < 3
        LLMFailed --> UseFallback : Max retries

        RetryLLM --> CallLLM
        UseFallback --> QuestionGenerated
        QuestionGenerated --> [*]
    }

    GeneratingQuestion --> ValidatingUniqueness : Question ready

    state ValidatingUniqueness {
        [*] --> TextSimilarity : Check text similarity
        TextSimilarity --> TopicOverlap : Check topic overlap
        TopicOverlap --> DuplicateCheck : Combine scores

        DuplicateCheck --> IsUnique : < 80% similar
        DuplicateCheck --> IsDuplicate : >= 80% similar

        IsUnique --> ValidationPassed
        IsDuplicate --> ValidationFailed

        ValidationPassed --> [*]
        ValidationFailed --> [*]
    }

    ValidatingUniqueness --> SavingHistory : Validation passed
    ValidatingUniqueness --> CheckingRetry : Validation failed

    state CheckingRetry {
        [*] --> CountRetries : Check retry count
        CountRetries --> CanRetry : < 3 attempts
        CountRetries --> MaxRetries : >= 3 attempts

        CanRetry --> [*]
        MaxRetries --> [*]
    }

    CheckingRetry --> GeneratingTopics : Can retry
    CheckingRetry --> ErrorHandling : Max retries

    state SavingHistory {
        [*] --> SaveToDB : Store question history
        SaveToDB --> UpdateProfile : Update user profile
        UpdateProfile --> HistorySaved
        HistorySaved --> [*]
    }

    SavingHistory --> Success : History saved

    state ErrorHandling {
        [*] --> LogError : Log error details
        LogError --> GenerateFallback : Create fallback
        GenerateFallback --> ErrorHandled
        ErrorHandled --> [*]
    }

    ErrorHandling --> Error : Error response

    Success --> [*] : Return question
    Error --> [*] : Return error

    note right of LoadingProfile : State: user_profile, resume_data
    note right of AnalyzingHistory : State: coverage_analysis, question_history
    note right of GeneratingTopics : State: candidate_topics[]
    note right of SelectingTopic : State: selected_topic
    note right of GeneratingQuestion : State: generated_question
    note right of ValidatingUniqueness : State: is_duplicate, similarity_score
    note right of SavingHistory : State: question_id, saved_at
```

### 3.3 ìƒíƒœ ë°ì´í„° í”Œë¡œìš°

```mermaid
graph LR
    subgraph "Initial State"
        IS[user_id<br/>resume_data<br/>target_position<br/>difficulty_preference<br/>excluded_categories]
    end

    subgraph "After LoadProfile"
        LP[+ user_profile<br/>+ parsing_confidence]
    end

    subgraph "After AnalyzeHistory"
        AH[+ question_history<br/>+ coverage_analysis<br/>+ gap_areas]
    end

    subgraph "After GenerateTopics"
        GT[+ candidate_topics<br/>+ priority_scores]
    end

    subgraph "After SelectTopic"
        ST[+ selected_topic<br/>+ reasoning]
    end

    subgraph "After GenerateQuestion"
        GQ[+ generated_question<br/>+ question_id<br/>+ metadata]
    end

    subgraph "After ValidateUniqueness"
        VU[+ is_duplicate<br/>+ similarity_score<br/>+ retry_count]
    end

    subgraph "Final State"
        FS[+ question_saved<br/>+ response_ready]
    end

    IS --> LP
    LP --> AH
    AH --> GT
    GT --> ST
    ST --> GQ
    GQ --> VU
    VU --> FS

    %% ì¡°ê±´ë¶€ ë£¨í”„
    VU -.->|duplicate & retry < 3| GT
    VU -.->|duplicate & retry >= 3| Error[Error State]
```

### 3.4 ìƒíƒœ ì „í™˜ ì¡°ê±´ ë§¤íŠ¸ë¦­ìŠ¤

```python
# ìƒíƒœ ì „í™˜ ì¡°ê±´ ì •ì˜
STATE_TRANSITIONS = {
    "load_user_profile": {
        "success_conditions": ["user_profile is not None"],
        "failure_conditions": ["user_id invalid", "database_error"],
        "next_states": {
            "success": "analyze_question_history",
            "failure": "handle_error"
        }
    },

    "analyze_question_history": {
        "success_conditions": ["coverage_analysis generated"],
        "next_states": {
            "success": "generate_candidate_topics"
        }
    },

    "generate_candidate_topics": {
        "success_conditions": ["len(candidate_topics) > 0"],
        "failure_conditions": ["no topics generated"],
        "next_states": {
            "success": "select_best_topic",
            "failure": "handle_error"
        }
    },

    "select_best_topic": {
        "success_conditions": ["selected_topic is not None"],
        "next_states": {
            "success": "generate_question"
        }
    },

    "generate_question": {
        "success_conditions": [
            "generated_question is not None",
            "question field exists",
            "category field valid"
        ],
        "failure_conditions": [
            "llm_api_error",
            "invalid_json_response",
            "rate_limit_exceeded"
        ],
        "next_states": {
            "success": "validate_uniqueness",
            "failure": "handle_error"
        }
    },

    "validate_uniqueness": {
        "branch_conditions": {
            "unique": [
                "similarity_score < 0.8",
                "topic_overlap < 0.7"
            ],
            "duplicate": [
                "similarity_score >= 0.8 OR topic_overlap >= 0.7",
                "retry_count < 3"
            ],
            "error": [
                "similarity_score >= 0.8 OR topic_overlap >= 0.7",
                "retry_count >= 3"
            ]
        },
        "next_states": {
            "unique": "save_question_history",
            "duplicate": "generate_candidate_topics",  # ì¬ì‹œë„
            "error": "handle_error"
        }
    },

    "save_question_history": {
        "success_conditions": ["question saved to database"],
        "failure_conditions": ["database_error"],
        "next_states": {
            "success": "END",
            "failure": "handle_error"
        }
    }
}

# ìƒíƒœ ê²€ì¦ í•¨ìˆ˜
def validate_state_transition(current_state: str, state_data: dict) -> str:
    """ìƒíƒœ ì „í™˜ ì¡°ê±´ ê²€ì¦"""

    transition_rules = STATE_TRANSITIONS.get(current_state, {})

    if current_state == "validate_uniqueness":
        # ë¶„ê¸° ì¡°ê±´ ê²€ì‚¬
        similarity = state_data.get("similarity_score", 0)
        topic_overlap = state_data.get("topic_overlap", 0)
        retry_count = state_data.get("retry_count", 0)

        if similarity < 0.8 and topic_overlap < 0.7:
            return "unique"
        elif retry_count >= 3:
            return "error"
        else:
            return "duplicate"

    # ì¼ë°˜ ì„±ê³µ/ì‹¤íŒ¨ ì¡°ê±´ ê²€ì‚¬
    success_conditions = transition_rules.get("success_conditions", [])
    failure_conditions = transition_rules.get("failure_conditions", [])

    # ì‹¤íŒ¨ ì¡°ê±´ ìš°ì„  ê²€ì‚¬
    for condition in failure_conditions:
        if check_condition(condition, state_data):
            return "failure"

    # ì„±ê³µ ì¡°ê±´ ê²€ì‚¬
    for condition in success_conditions:
        if not check_condition(condition, state_data):
            return "failure"

    return "success"

def check_condition(condition: str, state_data: dict) -> bool:
    """ì¡°ê±´ ê²€ì‚¬ ë¡œì§"""
    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ì¡°ê±´ íŒŒì‹± í•„ìš”
    if "is not None" in condition:
        field = condition.split()[0]
        return state_data.get(field) is not None
    elif "len(" in condition and "> 0" in condition:
        field = condition.split("(")[1].split(")")[0]
        return len(state_data.get(field, [])) > 0
    # ì¶”ê°€ ì¡°ê±´ë“¤...

    return True
```

### 3.5 í•µì‹¬ ë…¸ë“œ êµ¬í˜„

```python
async def generate_question_node(state: QuestionGenerationState) -> QuestionGenerationState:
    """LLMì„ ì‚¬ìš©í•œ ì§ˆë¬¸ ìƒì„± ë…¸ë“œ"""

    user_profile = state["user_profile"]
    selected_topic = state["selected_topic"]
    difficulty = state["difficulty_preference"]
    excluded_topics = state.get("excluded_topics", [])

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""
    ë‹¹ì‹ ì€ ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì¸í™”ëœ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

    ì§€ì›ì ì •ë³´:
    - ëª©í‘œ ì§ë¬´: {state['target_position']}
    - ê²½ë ¥ ìˆ˜ì¤€: {user_profile['experience_level']}
    - ì£¼ìš” ê¸°ìˆ : {user_profile['technical_skills']}
    - ì•½ì  ì˜ì—­: {user_profile['weak_areas']}

    ì§ˆë¬¸ ìš”êµ¬ì‚¬í•­:
    - ì£¼ì œ: {selected_topic['topic']}
    - ì¹´í…Œê³ ë¦¬: {selected_topic['category']}
    - ë‚œì´ë„: {difficulty} (0.0-1.0)
    - ì˜ˆìƒ ë‹µë³€ ì‹œê°„: 5-15ë¶„

    ì œì™¸í•  ì£¼ì œë“¤: {excluded_topics}

    ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
    {{
        "question": "ì§ˆë¬¸ ë‚´ìš©",
        "category": "technical|behavioral|situational|cultural",
        "difficulty": 0.7,
        "estimated_time": 10,
        "learning_objectives": ["ëª©í‘œ1", "ëª©í‘œ2"],
        "context": {{
            "background": "ì§ˆë¬¸ ë°°ê²½",
            "evaluation_criteria": ["í‰ê°€ ê¸°ì¤€1", "í‰ê°€ ê¸°ì¤€2"]
        }}
    }}
    """

    try:
        # Claude API í˜¸ì¶œ
        response = await claude_api.generate_question(
            prompt=prompt,
            temperature=0.7,
            max_tokens=500
        )

        generated_question = json.loads(response.content)

        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        generated_question.update({
            "question_id": str(uuid.uuid4()),
            "generated_at": datetime.now().isoformat(),
            "user_id": state["user_id"],
            "topic_source": selected_topic
        })

        return {
            **state,
            "generated_question": generated_question
        }

    except Exception as e:
        return {
            **state,
            "error": f"Question generation failed: {str(e)}"
        }

async def analyze_history_node(state: QuestionGenerationState) -> QuestionGenerationState:
    """ì§ˆë¬¸ ì´ë ¥ ë¶„ì„ ë…¸ë“œ"""

    question_history = state["question_history"]
    user_profile = state["user_profile"]

    if not question_history:
        # ì²« ì§ˆë¬¸ì¸ ê²½ìš° ê¸°ë³¸ ë¶„ì„
        coverage_analysis = {
            "total_questions": 0,
            "category_distribution": {},
            "covered_topics": [],
            "gap_areas": user_profile.get("weak_areas", []),
            "recommended_focus": "technical"  # ê¸°ë³¸ê°’
        }
    else:
        # ê¸°ì¡´ ì§ˆë¬¸ë“¤ ë¶„ì„
        coverage_analysis = {
            "total_questions": len(question_history),
            "category_distribution": calculate_category_distribution(question_history),
            "covered_topics": extract_covered_topics(question_history),
            "gap_areas": identify_uncovered_areas(question_history, user_profile),
            "recent_difficulty_trend": analyze_difficulty_trend(question_history[-5:])
        }

    return {
        **state,
        "coverage_analysis": coverage_analysis
    }

async def generate_topics_node(state: QuestionGenerationState) -> QuestionGenerationState:
    """í›„ë³´ ì£¼ì œ ìƒì„± ë…¸ë“œ"""

    user_profile = state["user_profile"]
    coverage_analysis = state["coverage_analysis"]
    excluded_topics = state.get("excluded_topics", [])

    # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì£¼ì œ ìƒì„±
    candidate_topics = []

    # 1. ì•½ì  ë³´ì™„ ì£¼ì œë“¤
    for weak_area in coverage_analysis["gap_areas"]:
        if weak_area not in excluded_topics:
            candidate_topics.append({
                "topic": weak_area,
                "category": categorize_topic(weak_area),
                "priority": 0.9,
                "reasoning": f"ì•½ì  ë³´ì™„: {weak_area}"
            })

    # 2. ê²½ë ¥ ê¸°ë°˜ ì£¼ì œë“¤
    for experience in user_profile.get("experience", []):
        tech_stack = experience.get("technologies", [])
        for tech in tech_stack:
            if tech not in excluded_topics:
                candidate_topics.append({
                    "topic": f"{tech} ì‹¬í™” ì§ˆë¬¸",
                    "category": "technical",
                    "priority": 0.7,
                    "reasoning": f"ê²½ë ¥ ê¸°ë°˜: {tech} ê²½í—˜ í™œìš©"
                })

    # 3. ê· í˜• ë§ì¶”ê¸° ì£¼ì œë“¤
    underrepresented_categories = find_underrepresented_categories(
        coverage_analysis["category_distribution"]
    )

    for category in underrepresented_categories:
        candidate_topics.append({
            "topic": f"{category} ì¼ë°˜ ì§ˆë¬¸",
            "category": category,
            "priority": 0.6,
            "reasoning": f"ì¹´í…Œê³ ë¦¬ ê· í˜•: {category} ë¶€ì¡±"
        })

    # ìš°ì„ ìˆœìœ„ ì •ë ¬
    candidate_topics.sort(key=lambda x: x["priority"], reverse=True)

    return {
        **state,
        "candidate_topics": candidate_topics[:10]  # ìƒìœ„ 10ê°œë§Œ
    }
```

### 3.2 ì§ˆë¬¸ ìœ í˜•ë³„ ë¶„ë°°

```yaml
question_distribution:
  week_1:  # ê¸°ì´ˆ ì ì‘ (Day 1-7)
    technical: 40%
    behavioral: 35%
    situational: 15%
    cultural: 10%
    focus: "ê¸°ë³¸ ê°œë… í™•ì¸, ê²½í—˜ ê³µìœ "

  week_2:  # ì‹¤ë¬´ ì ìš© (Day 8-14)
    technical: 45%
    behavioral: 30%
    situational: 20%
    cultural: 5%
    focus: "ì‹¤ë¬´ ê²½í—˜, ë¬¸ì œ í•´ê²°"

  week_3:  # ì‹¬í™” ë¶„ì„ (Day 15-21)
    technical: 50%
    behavioral: 25%
    situational: 20%
    cultural: 5%
    focus: "ì‹œìŠ¤í…œ ì„¤ê³„, ì•„í‚¤í…ì²˜"

  week_4:  # ê³ ê¸‰ ì¢…í•© (Day 22-28)
    technical: 40%
    behavioral: 30%
    situational: 25%
    cultural: 5%
    focus: "ë¦¬ë”ì‹­, ë¹„ì¦ˆë‹ˆìŠ¤ ì´í•´"

  final_days:  # ìµœì¢… ì ê²€ (Day 29-30)
    review_weak_areas: 80%
    comprehensive: 20%
    focus: "ì•½ì  ë³´ì™„, ì¢…í•© ì •ë¦¬"
```

## 4. ë‹µë³€ ë¶„ì„ ë° í”¼ë“œë°± ì‹œìŠ¤í…œ

### 4.1 ë‹µë³€ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°

```mermaid
graph LR
    Answer([ì‚¬ìš©ì ë‹µë³€ ì œì¶œ]) --> Validate{ë‹µë³€ ê²€ì¦}
    Validate -->|Valid| Analyze[LLM ë‹µë³€ ë¶„ì„]
    Validate -->|Invalid| Request[ì¬ì‘ì„± ìš”ì²­]

    Analyze --> MultiLLM{ë‹¤ì¤‘ LLM í‰ê°€}
    MultiLLM --> Claude[Claude<br/>êµ¬ì¡°/ë…¼ë¦¬ì„±]
    MultiLLM --> GPT4[GPT-4<br/>ê¸°ìˆ /ì •í™•ì„±]
    MultiLLM --> Gemini[Gemini<br/>í‘œí˜„/ì†Œí†µ]

    Claude --> Integrate[ì ìˆ˜ í†µí•©]
    GPT4 --> Integrate
    Gemini --> Integrate

    Integrate --> GenerateFeedback[ë§ì¶¤ í”¼ë“œë°± ìƒì„±]
    GenerateFeedback --> SendFeedback[í”¼ë“œë°± ì „ì†¡]
    SendFeedback --> UpdateProgress[ì§„í–‰ë„ ì—…ë°ì´íŠ¸]
```

### 4.2 í”¼ë“œë°± ìƒì„± ë¡œì§

```python
async def generate_comprehensive_feedback(question, answer, user_profile):
    """ì¢…í•©ì ì¸ í”¼ë“œë°± ìƒì„±"""

    # 1. ë‹¤ì¤‘ LLM í‰ê°€
    evaluations = await asyncio.gather(
        claude_evaluate(question, answer, focus="structure"),
        gpt4_evaluate(question, answer, focus="technical"),
        gemini_evaluate(question, answer, focus="communication")
    )

    # 2. ì ìˆ˜ í†µí•©
    integrated_score = integrate_scores(evaluations)

    # 3. ê°œì¸í™”ëœ í”¼ë“œë°± ìƒì„±
    feedback = {
        "overall_score": integrated_score["total"],
        "dimension_scores": {
            "structure": integrated_score["structure"],
            "technical": integrated_score["technical"],
            "communication": integrated_score["communication"],
            "completeness": integrated_score["completeness"]
        },
        "strengths": extract_strengths(evaluations),
        "improvements": extract_improvements(evaluations),
        "specific_tips": generate_specific_tips(question, answer, user_profile),
        "next_focus_areas": determine_next_focus(user_profile, integrated_score),
        "similar_questions": recommend_practice_questions(question, integrated_score)
    }

    return feedback

def generate_specific_tips(question, answer, user_profile):
    """êµ¬ì²´ì ì¸ ê°œì„  íŒ ìƒì„±"""
    tips = []

    # ë‹µë³€ ê¸¸ì´ ë¶„ì„
    if len(answer.split()) < 50:
        tips.append("ë‹µë³€ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ë³´ì„¸ìš”. êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ ìˆ˜ì¹˜ë¥¼ í¬í•¨í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤.")

    # STAR ë©”ì†Œë“œ ì ìš© ì—¬ë¶€
    if question_requires_star_method(question) and not has_star_structure(answer):
        tips.append("STAR ë©”ì†Œë“œ(Situation, Task, Action, Result)ë¥¼ í™œìš©í•´ êµ¬ì¡°ì ìœ¼ë¡œ ë‹µë³€í•´ë³´ì„¸ìš”.")

    # ê¸°ìˆ ì  ê¹Šì´
    if is_technical_question(question) and lacks_technical_depth(answer):
        tips.append("ê¸°ìˆ ì  ê°œë…ì„ ë” ìƒì„¸íˆ ì„¤ëª…í•˜ê³ , ì™œ ê·¸ëŸ° ì„ íƒì„ í–ˆëŠ”ì§€ ì´ìœ ë¥¼ í¬í•¨í•˜ì„¸ìš”.")

    return tips
```

## 5. ì§„í–‰ë„ ì¶”ì  ë° ëŒ€ì‹œë³´ë“œ

### 5.1 í•™ìŠµ ì§„í–‰ë„ ì‹œê°í™”

```mermaid
graph TB
    subgraph "ì£¼ê°„ ì§„í–‰ë„"
        W1[Week 1<br/>ê¸°ì´ˆ ì ì‘<br/>7/7 ì™„ë£Œ]
        W2[Week 2<br/>ì‹¤ë¬´ ì ìš©<br/>5/7 ì§„í–‰ì¤‘]
        W3[Week 3<br/>ì‹¬í™” ë¶„ì„<br/>0/7 ëŒ€ê¸°]
        W4[Week 4<br/>ê³ ê¸‰ ì¢…í•©<br/>0/7 ëŒ€ê¸°]
    end

    subgraph "ì ìˆ˜ íŠ¸ë Œë“œ"
        Score1[Day 1: 65ì ]
        Score7[Day 7: 78ì ]
        Score12[Day 12: 82ì ]
        Trend[ğŸ“ˆ ìƒìŠ¹ ì¶”ì„¸]
    end

    subgraph "ê°•ì /ì•½ì  ë¶„ì„"
        Strong[ê°•ì <br/>â€¢ ê¸°ìˆ  ì§€ì‹<br/>â€¢ ë¬¸ì œ í•´ê²°]
        Weak[ì•½ì <br/>â€¢ êµ¬ì¡°ì  ë‹µë³€<br/>â€¢ ë¦¬ë”ì‹­ ê²½í—˜]
    end
```

### 5.2 ê°œì¸í™”ëœ ëŒ€ì‹œë³´ë“œ API

```python
@app.get("/dashboard/{user_id}")
async def get_user_dashboard(user_id: str):
    """ì‚¬ìš©ì ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì¡°íšŒ"""

    user_progress = await get_user_progress(user_id)

    return {
        "current_day": user_progress["study_day"],
        "completion_rate": user_progress["completion_rate"],
        "average_score": user_progress["average_score"],
        "score_trend": user_progress["score_history"][-7:],  # ìµœê·¼ 7ì¼
        "strengths": user_progress["identified_strengths"],
        "improvement_areas": user_progress["improvement_areas"],
        "streak_days": user_progress["consecutive_days"],
        "next_milestone": calculate_next_milestone(user_progress),
        "recommendations": generate_study_recommendations(user_progress)
    }

@app.get("/analytics/{user_id}")
async def get_detailed_analytics(user_id: str):
    """ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸"""

    return {
        "category_performance": {
            "technical": calculate_category_score(user_id, "technical"),
            "behavioral": calculate_category_score(user_id, "behavioral"),
            "situational": calculate_category_score(user_id, "situational"),
            "cultural": calculate_category_score(user_id, "cultural")
        },
        "difficulty_progression": analyze_difficulty_progression(user_id),
        "response_patterns": analyze_response_patterns(user_id),
        "improvement_velocity": calculate_improvement_rate(user_id),
        "peer_comparison": get_anonymized_peer_stats(user_id)
    }
```

## 6. ì•Œë¦¼ ë° ì°¸ì—¬ë„ ê´€ë¦¬

### 6.1 ìŠ¤ë§ˆíŠ¸ ë¦¬ë§ˆì¸ë” ì‹œìŠ¤í…œ

```yaml
reminder_strategy:
  first_reminder:
    delay: 12_hours
    channel: email
    tone: friendly
    message: "ì˜¤ëŠ˜ì˜ ë©´ì ‘ ì§ˆë¬¸ì— ë‹µë³€í•´ë³´ì„¸ìš”!"

  second_reminder:
    delay: 20_hours
    channel: [email, slack, push]
    tone: encouraging
    message: "ë©´ì ‘ ì¤€ë¹„ì˜ ì—°ì†ì„±ì´ ì¤‘ìš”í•´ìš”. ì§§ê²Œë¼ë„ ë‹µë³€í•´ë³´ì„¸ìš”!"

  missed_day:
    action: adjust_difficulty
    next_question: easier
    personalized_message: true

  streak_break:
    action: motivational_content
    include: progress_summary
    suggest: comeback_plan
```

### 6.2 ì°¸ì—¬ë„ í–¥ìƒ ì „ëµ

```python
class EngagementManager:
    def __init__(self):
        self.strategies = {
            "gamification": GamificationEngine(),
            "social": SocialFeatures(),
            "personalization": PersonalizationEngine()
        }

    async def maintain_engagement(self, user_id: str):
        user_data = await self.get_user_data(user_id)

        # ì—°ì† ì°¸ì—¬ì¼ ì¶”ì 
        if user_data["streak_days"] >= 7:
            await self.send_achievement_notification(user_id, "week_warrior")

        # ì°¸ì—¬ë„ í•˜ë½ ê°ì§€
        if user_data["recent_engagement"] < 0.5:
            await self.trigger_re_engagement_flow(user_id)

        # ê°œì¸í™”ëœ ë™ê¸°ë¶€ì—¬
        if user_data["improvement_stagnation"]:
            await self.adjust_question_strategy(user_id)

    async def trigger_re_engagement_flow(self, user_id: str):
        """ì°¸ì—¬ë„ íšŒë³µ í”Œë¡œìš°"""
        strategies = [
            "send_progress_summary",
            "reduce_question_difficulty",
            "offer_bonus_content",
            "request_feedback_on_service"
        ]

        for strategy in strategies:
            await self.execute_strategy(user_id, strategy)

            # 24ì‹œê°„ í›„ ì°¸ì—¬ë„ í™•ì¸
            await asyncio.sleep(86400)
            if await self.check_engagement_recovery(user_id):
                break
```

## 7. í™•ì¥ ê¸°ëŠ¥

### 7.1 ê·¸ë£¹ ì±Œë¦°ì§€

```mermaid
graph TB
    Challenge[30ì¼ ë©´ì ‘ ì±Œë¦°ì§€] --> Team[íŒ€ êµ¬ì„±<br/>5-10ëª…]
    Team --> Daily[ë§¤ì¼ ë™ì¼ ì§ˆë¬¸]
    Daily --> Share[ë‹µë³€ ê³µìœ <br/>ì˜µì…˜]
    Share --> Vote[ë™ë£Œ í‰ê°€]
    Vote --> Leaderboard[ë¦¬ë”ë³´ë“œ]
    Leaderboard --> Prize[ì™„ì£¼ ë³´ìƒ]
```

### 7.2 ë©´ì ‘ê´€ í”¼ë“œë°± ëª¨ë“œ

```python
async def interviewer_feedback_mode(question, answer, interviewer_profile):
    """ì‹¤ì œ ë©´ì ‘ê´€ ìŠ¤íƒ€ì¼ í”¼ë“œë°±"""

    # ë©´ì ‘ê´€ í˜ë¥´ì†Œë‚˜ ì ìš©
    interviewer_prompt = f"""
    ë‹¹ì‹ ì€ {interviewer_profile['company']}ì˜ {interviewer_profile['position']} ë©´ì ‘ê´€ì…ë‹ˆë‹¤.
    ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì§€ì›ìì˜ ë‹µë³€ì„ í‰ê°€í•˜ê³  ì‹¤ì œ ë©´ì ‘ì²˜ëŸ¼ í”¼ë“œë°±í•´ì£¼ì„¸ìš”.

    ë©´ì ‘ê´€ íŠ¹ì„±:
    - ê²½ë ¥: {interviewer_profile['experience']}ë…„
    - í‰ê°€ ìŠ¤íƒ€ì¼: {interviewer_profile['evaluation_style']}
    - ì¤‘ìš”ì‹œí•˜ëŠ” ê°€ì¹˜: {interviewer_profile['values']}
    """

    feedback = await llm.generate_feedback(
        prompt=interviewer_prompt,
        question=question,
        answer=answer
    )

    return {
        "interviewer_persona": interviewer_profile,
        "feedback": feedback,
        "likely_follow_ups": generate_follow_up_questions(question, answer),
        "hiring_probability": estimate_hiring_probability(feedback)
    }
```

### 7.3 ì—…ê³„ë³„ íŠ¹í™” ëª¨ë“œ

```yaml
industry_specialization:
  tech_startup:
    focus_areas: ["ê¸°ìˆ  ê¹Šì´", "ë¹ ë¥¸ ì‹¤í–‰ë ¥", "ë¬¸ì œ í•´ê²°"]
    common_questions: ["MVP ê°œë°œ ê²½í—˜", "ê¸°ìˆ  ë¶€ì±„ ê´€ë¦¬", "ìŠ¤íƒ€íŠ¸ì—… ë¬¸í™” ì ì‘"]

  big_tech:
    focus_areas: ["ì‹œìŠ¤í…œ ì„¤ê³„", "í™•ì¥ì„±", "ì•Œê³ ë¦¬ì¦˜"]
    common_questions: ["ëŒ€ê·œëª¨ ì‹œìŠ¤í…œ", "ì„±ëŠ¥ ìµœì í™”", "ë¶„ì‚° ì²˜ë¦¬"]

  consulting:
    focus_areas: ["êµ¬ì¡°ì  ì‚¬ê³ ", "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", "ë¹„ì¦ˆë‹ˆìŠ¤ ì„¼ìŠ¤"]
    common_questions: ["ì¼€ì´ìŠ¤ ìŠ¤í„°ë””", "í´ë¼ì´ì–¸íŠ¸ ê´€ë¦¬", "í”„ë¡œì íŠ¸ ë¦¬ë”©"]

  finance:
    focus_areas: ["ìœ„í—˜ ê´€ë¦¬", "ì •í™•ì„±", "ê·œì œ ì¤€ìˆ˜"]
    common_questions: ["ê¸ˆìœµ ëª¨ë¸ë§", "ë¦¬ìŠ¤í¬ ë¶„ì„", "ê·œì œ ëŒ€ì‘"]
```

## 4. ì™¸ë¶€ ì„œë²„ ì—°ë™ ì„¤ê³„

### 4.1 API í˜¸ì¶œ ì‹œë‚˜ë¦¬ì˜¤

```python
# ì™¸ë¶€ ìŠ¤ì¼€ì¤„ë§ ì„œë²„ì—ì„œì˜ í˜¸ì¶œ ì˜ˆì‹œ
import httpx
import asyncio

class InterviewQuestionService:
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.client = httpx.AsyncClient()

    async def get_daily_question(self, user_id: str, target_position: str, resume_id: Optional[str] = None, session_id: Optional[str] = None):
        """ë§¤ì¼ ìŠ¤ì¼€ì¤„ë§ì—ì„œ í˜¸ì¶œí•  ì§ˆë¬¸ ìƒì„±"""

        # 1. ì§ˆë¬¸ ìƒì„± ìš”ì²­
        response = await self.client.post(
            f"{self.base_url}/generate-question",
            json={
                "user_id": user_id,
                "resume_id": resume_id,  # íŠ¹ì • ì´ë ¥ì„œ ì§€ì •
                "session_id": session_id,  # ë˜ëŠ” ì„¸ì…˜ ì§€ì •
                "target_position": target_position,
                "difficulty_preference": 0.7,
                "excluded_categories": []
            }
        )

        if response.status_code == 200:
            question_data = response.json()
            return {
                "question": question_data["question"],
                "category": question_data["category"],
                "difficulty": question_data["difficulty"],
                "estimated_time": question_data["estimated_time"],
                "question_id": question_data["question_id"]
            }
        else:
            # í´ë°± ë¡œì§
            return await self.get_fallback_question(target_position)

    async def initialize_user(self, user_id: str, resume_path: str):
        """ì‹ ê·œ ì‚¬ìš©ì ì´ˆê¸°í™”"""
        response = await self.client.post(
            f"{self.base_url}/user/{user_id}/initialize",
            json={"resume_file": resume_path}
        )
        return response.json()

# ìŠ¤ì¼€ì¤„ë§ ì„œë²„ì˜ ì¼ì¼ ì‹¤í–‰ ë¡œì§
async def daily_scheduled_process():
    question_service = InterviewQuestionService("http://question-api:8000")

    # í™œì„± ì‚¬ìš©ì ì¡°íšŒ (ìŠ¤ì¼€ì¤„ë§ ì„œë²„ì˜ DBì—ì„œ)
    active_users = await get_active_users()

    for user in active_users:
        try:
            # ì§ˆë¬¸ ìƒì„± API í˜¸ì¶œ
            question = await question_service.get_daily_question(
                user_id=user["id"],
                target_position=user["target_position"]
            )

            # ë©”ì‹œì§€ ë°œì†¡ (ì´ë©”ì¼/ìŠ¬ë™/í‘¸ì‹œ)
            await send_question_notification(user, question)

            # ë°œì†¡ ì´ë ¥ ì €ì¥
            await save_sent_history(user["id"], question)

        except Exception as e:
            logger.error(f"Failed to process user {user['id']}: {e}")
            # ì—ëŸ¬ ì•Œë¦¼ ë˜ëŠ” ì¬ì‹œë„ ë¡œì§
```

### 4.2 ë‹¤ì¤‘ ì´ë ¥ì„œ ì§€ì› DB ì„¤ê³„

```yaml
# ì§ˆë¬¸ ìƒì„± ì„œë¹„ìŠ¤ DB (PostgreSQL)
question_service_db:
  tables:
    users:
      - user_id (UUID, PK)
      - created_at (TIMESTAMP)
      - updated_at (TIMESTAMP)

    resumes:
      - resume_id (UUID, PK)
      - user_id (UUID, FK -> users.user_id)
      - file_name (VARCHAR)
      - file_path (TEXT)
      - parsed_data (JSONB)
      - parsing_confidence (FLOAT)
      - is_primary (BOOLEAN)  # ê¸°ë³¸ ì´ë ¥ì„œ ì—¬ë¶€
      - created_at (TIMESTAMP)
      - updated_at (TIMESTAMP)

    user_profiles:
      - profile_id (UUID, PK)
      - user_id (UUID, FK -> users.user_id)
      - resume_id (UUID, FK -> resumes.resume_id)
      - target_position (VARCHAR)
      - experience_level (VARCHAR)
      - skill_gaps (TEXT[])
      - strong_areas (TEXT[])
      - resume_analysis (JSONB)
      - created_at (TIMESTAMP)

    question_history:
      - id (UUID, PK)
      - user_id (UUID, FK -> users.user_id)
      - resume_id (UUID, FK -> resumes.resume_id)
      - question_id (UUID)
      - question (TEXT)
      - category (VARCHAR)
      - difficulty (FLOAT)
      - learning_objectives (TEXT[])
      - generated_at (TIMESTAMP)

    question_sessions:  # ì§ˆë¬¸ ì„¸ì…˜ ê´€ë¦¬ (ê°™ì€ ì»¨í…ìŠ¤íŠ¸)
      - session_id (UUID, PK)
      - user_id (UUID, FK -> users.user_id)
      - resume_id (UUID, FK -> resumes.resume_id)
      - target_position (VARCHAR)
      - session_name (VARCHAR)  # "ë°±ì—”ë“œ ë©´ì ‘ ì¤€ë¹„", "í”„ë¡ íŠ¸ì—”ë“œ ì „í™˜" ë“±
      - created_at (TIMESTAMP)
      - is_active (BOOLEAN)

# ìŠ¤ì¼€ì¤„ë§ ì„œë²„ DB (ë³„ë„ ì¸ìŠ¤í„´ìŠ¤)
scheduling_service_db:
  tables:
    users:
      - id (UUID)
      - email (VARCHAR)
      - slack_channel (VARCHAR)
      - target_position (VARCHAR)
      - status (ENUM: active, paused, completed)
      - created_at (TIMESTAMP)

    sent_questions:
      - id (UUID)
      - user_id (UUID)
      - question_id (UUID)
      - sent_at (TIMESTAMP)
      - delivery_status (ENUM: sent, failed, delivered)

    user_responses:
      - id (UUID)
      - user_id (UUID)
      - question_id (UUID)
      - response (TEXT)
      - submitted_at (TIMESTAMP)
```

### 4.3 ì—ëŸ¬ ì²˜ë¦¬ ë° í´ë°±

```python
class QuestionGenerationError(Exception):
    pass

class FallbackQuestionProvider:
    def __init__(self):
        self.generic_questions = {
            "technical": [
                "ìµœê·¼ì— í•´ê²°í•œ ê¸°ìˆ ì  ë¬¸ì œì™€ í•´ê²° ê³¼ì •ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "ì‚¬ìš©í•´ë³¸ ê¸°ìˆ  ìŠ¤íƒ ì¤‘ ê°€ì¥ ì¸ìƒ ê¹Šì—ˆë˜ ê²ƒê³¼ ì´ìœ ëŠ”?",
            ],
            "behavioral": [
                "íŒ€ì—ì„œ ê°ˆë“±ì´ ìˆì—ˆë˜ ê²½í—˜ê³¼ í•´ê²° ë°©ë²•ì„ ê³µìœ í•´ì£¼ì„¸ìš”.",
                "ì‹¤íŒ¨í•œ í”„ë¡œì íŠ¸ ê²½í—˜ê³¼ ê·¸ë¡œë¶€í„° ë°°ìš´ ì ì€?",
            ]
        }

    def get_fallback_question(self, category: str = "technical"):
        import random
        questions = self.generic_questions.get(category, self.generic_questions["technical"])
        return {
            "question": random.choice(questions),
            "category": category,
            "difficulty": 0.5,
            "estimated_time": 10,
            "question_id": f"fallback-{uuid.uuid4()}",
            "is_fallback": True
        }

# API ì„œë²„ì˜ ì—ëŸ¬ í•¸ë“¤ë§
@app.post("/generate-question")
async def generate_question_with_fallback(request: QuestionRequest):
    try:
        # ì •ìƒ ì§ˆë¬¸ ìƒì„± ë¡œì§
        result = await generate_question_workflow(request)
        return result

    except QuestionGenerationError as e:
        logger.warning(f"Question generation failed for user {request.user_id}: {e}")

        # í´ë°± ì§ˆë¬¸ ì œê³µ
        fallback_provider = FallbackQuestionProvider()
        fallback_question = fallback_provider.get_fallback_question()

        return QuestionResponse(**fallback_question)

    except Exception as e:
        logger.error(f"Unexpected error for user {request.user_id}: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")
```

## 5. êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: API ì„œë¹„ìŠ¤ MVP (3ì£¼)
- [ ] LangGraph ì§ˆë¬¸ ìƒì„± ì›Œí¬í”Œë¡œìš° êµ¬í˜„
- [ ] Dolphin ì´ë ¥ì„œ íŒŒì‹± ì—°ë™
- [ ] ì¤‘ë³µ ë°©ì§€ ë¡œì§ (í…ìŠ¤íŠ¸ ìœ ì‚¬ë„)
- [ ] FastAPI ì„œë²„ & ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸
- [ ] PostgreSQL ìŠ¤í‚¤ë§ˆ ì„¤ê³„

### Phase 2: ê³ ë„í™” (4ì£¼)
- [ ] ë‹¤ì¤‘ LLM í†µí•© (Claude, GPT-4, Gemini)
- [ ] ê³ ê¸‰ ì¤‘ë³µ ê²€ì¦ (ì„ë² ë”© ê¸°ë°˜)
- [ ] ì‚¬ìš©ì í”„ë¡œí•„ ê°œì¸í™” ì•Œê³ ë¦¬ì¦˜
- [ ] Redis ìºì‹± ì‹œìŠ¤í…œ
- [ ] ì—ëŸ¬ ì²˜ë¦¬ & í´ë°± ì‹œìŠ¤í…œ

### Phase 3: í™•ì¥ & ìµœì í™” (5ì£¼)
- [ ] ì§ˆë¬¸ í’ˆì§ˆ ê°œì„  (A/B í…ŒìŠ¤íŠ¸)
- [ ] ì„±ëŠ¥ ìµœì í™” (ë°°ì¹˜ ì²˜ë¦¬)
- [ ] ëª¨ë‹ˆí„°ë§ & ë¡œê¹… ì‹œìŠ¤í…œ
- [ ] Docker ì»¨í…Œì´ë„ˆí™”
- [ ] Kubernetes ë°°í¬ ì„¤ì •

### Phase 4: ìš´ì˜ & ê°œì„  (ì§€ì†)
- [ ] ì§ˆë¬¸ í’ˆì§ˆ í”¼ë“œë°± ë£¨í”„
- [ ] ì‚¬ìš© íŒ¨í„´ ë¶„ì„ & ìµœì í™”
- [ ] ìƒˆë¡œìš´ ì§ˆë¬¸ ì¹´í…Œê³ ë¦¬ ì¶”ê°€
- [ ] ì—…ê³„ë³„ íŠ¹í™” ëª¨ë“œ

## ì°¸ê³  ìë£Œ
- Dolphin ë…¼ë¬¸: https://arxiv.org/abs/2406.18842
- HuggingFace ëª¨ë¸: https://huggingface.co/ByteDance/Dolphin
- n8n í¬ë¡ ì¡ ë¬¸ì„œ: https://docs.n8n.io/nodes/n8n-nodes-base.cron/
- í–‰ë™ë©´ì ‘ STAR ë©”ì†Œë“œ: https://www.indeed.com/career-advice/interviewing/how-to-use-the-star-method