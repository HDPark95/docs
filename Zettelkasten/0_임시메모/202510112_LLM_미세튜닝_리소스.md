---
생성일: 2025-10-11
태그: #LLM #미세튜닝 #Ollama #리소스 #학습
상태: 임시메모
---

# Ollama 모델 미세 튜닝 리소스 요구사항

## 질문
Ollama로 다운로드한 모델을 실제 미세 튜닝(가중치 재학습)하려면 어느 정도 리소스가 필요한가?

## 리소스 요구사항

### 1. Full Fine-tuning (전체 재학습)

#### 7B 모델 기준
- **GPU 메모리**: 최소 24GB (RTX 3090/4090)
  - 모델 가중치: ~14GB
  - 그래디언트: ~14GB
  - 옵티마이저 상태: ~14GB
  - 총 필요: 40GB+ (실제로는 A100 40GB 이상 권장)
- **시스템 RAM**: 32GB 이상
- **저장 공간**: 50GB 이상
- **학습 시간**: 수 시간 ~ 수일 (데이터셋 크기에 따라)

#### 13B 모델 기준
- **GPU 메모리**: 48GB 이상 (A100 80GB 권장)
- **시스템 RAM**: 64GB 이상
- **저장 공간**: 100GB 이상

**현실**: 개인 PC에서는 거의 불가능

### 2. LoRA (Low-Rank Adaptation) - 추천

#### 7B 모델 기준
- **GPU 메모리**: 12-16GB (RTX 3060 12GB, RTX 4060 Ti 16GB)
  - 원본 모델: 동결 (메모리 절약)
  - 학습 파라미터: 1% 미만 (~100MB)
  - 배치 크기 조정으로 10GB 이하도 가능
- **시스템 RAM**: 16GB 이상
- **저장 공간**: 20GB
- **학습 시간**: 30분 ~ 수 시간

#### 13B 모델 기준
- **GPU 메모리**: 20-24GB (RTX 3090/4090)
- **시스템 RAM**: 32GB 이상

**현실**: 개인 PC에서 충분히 가능 (RTX 3060 이상)

### 3. QLoRA (Quantized LoRA) - 최적화

#### 7B 모델 기준
- **GPU 메모리**: 6-8GB (RTX 3060 Ti, RTX 4060)
  - 4-bit 양자화로 메모리 75% 절감
  - 8GB GPU로도 학습 가능
- **시스템 RAM**: 16GB
- **저장 공간**: 15GB
- **학습 시간**: LoRA보다 약간 느림

#### 13B 모델 기준
- **GPU 메모리**: 12-16GB
- **시스템 RAM**: 32GB

**현실**: 일반 게이밍 PC에서도 가능 (GTX 1080 Ti 이상)

## 실용적 선택지

### 개인 개발자 추천 순위

1. **QLoRA** (가성비 최고)
   - RTX 3060 8GB만 있어도 7B 모델 학습 가능
   - Unsloth 라이브러리 사용 시 속도 2-5배 향상
   - 품질: Full Fine-tuning의 95% 수준

2. **LoRA** (성능/속도 균형)
   - RTX 3060 12GB 이상 권장
   - 안정적이고 검증된 방법
   - 커뮤니티 지원 풍부

3. **Full Fine-tuning** (비추천)
   - 클라우드 GPU 사용 (시간당 $2-5)
   - 연구/실험용으로만 고려

## 필요 도구

```bash
# GPU 확인
nvidia-smi

# PyTorch + CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# LoRA 학습
pip install peft transformers datasets accelerate bitsandbytes

# Unsloth (QLoRA 최적화)
pip install unsloth
```

## 학습 프로세스 예시 (QLoRA)

```python
from unsloth import FastLanguageModel

# 모델 로드 (4-bit 양자화)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="llama-2-7b",  # Ollama에서 export한 모델
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True,
)

# LoRA 어댑터 추가
model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # LoRA rank
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj"],
)

# 학습 (8GB GPU에서 실행 가능)
trainer.train()
```

## 비용 비교

| 방법 | 개인 PC | 클라우드 (AWS) |
|-----|---------|---------------|
| QLoRA (7B) | RTX 3060 8GB ($300) | p3.2xlarge $3/hr × 2hr = $6 |
| LoRA (7B) | RTX 3060 12GB ($400) | p3.2xlarge $3/hr × 1hr = $3 |
| Full (7B) | 불가능 | p3.8xlarge $12/hr × 10hr = $120 |

## 현실적인 시작점

**추천 환경**:
- GPU: RTX 3060 12GB 이상
- RAM: 16GB
- 방법: QLoRA + Unsloth
- 모델: Llama 2/3 7B, Mistral 7B

**학습 데이터셋**: 100-1000개 샘플로 시작

## 연결 개념

- Parameter-Efficient Fine-Tuning (PEFT)
- Quantization (4-bit, 8-bit)
- Adapter Layers
- Transfer Learning
- Model Compression
