2장+3장 집중 실습 예제 - 규모 추정 + 면접 접근법

이 문서는 2장(개략적 규모 추정)과 3장(시스템 설계 면접 공략법)을 동시에 연습하기 위한 예제입니다.
각 예제는 반드시 규모 추정을 먼저 수행한 후 4단계 접근법을 따라 진행합니다.

---

## 실습 템플릿

각 예제를 시작하기 전, 이 순서를 따르세요:

### 규모 추정 단계 (10분)
1. 일간 능동 사용자(DAU) 계산
2. QPS (읽기/쓰기 분리)
3. 피크 QPS (평균의 2-3배)
4. 저장 용량 (1일, 1개월, 1년, 5년)
5. 대역폭 (초당 전송량)
6. 메모리 캐시 용량 (80/20 법칙)

### 4단계 면접 접근법 (35-40분)
1. 문제 이해 및 설계 범위 확정 (5-10분)
2. 개략적 설계안 제시 및 동의 구하기 (10-15분)
3. 상세 설계 (15-20분)
4. 마무리 (5분)

---

## 예제 1: Instagram 스토리 기능 (난이도: 중)

문제
"Instagram/Snapchat 스토리(24시간 후 자동 삭제)를 설계하세요."

### STEP 0: 규모 추정 (실습 필수!)

주어진 조건
- 월간 능동 사용자(MAU): 10억 명
- 일간 능동 사용자(DAU): 5억 명 (MAU의 50%)
- 스토리 업로드: 사용자당 하루 평균 2개
- 스토리 조회: 사용자당 하루 평균 50개
- 이미지 크기: 평균 500KB
- 동영상 크기: 평균 3MB
- 이미지/동영상 비율: 7:3
- 데이터 보관 기간: 24시간

계산해야 할 항목
[ ] 일간 스토리 업로드 수
[ ] 쓰기 QPS (평균, 피크)
[ ] 읽기 QPS (평균, 피크)
[ ] 일일 저장 용량
[ ] 피크 시간대 대역폭 (업로드/다운로드)
[ ] 캐시 메모리 용량 (인기 스토리 20%)

예상 결과 (스스로 계산 후 확인)
- 일간 업로드: 5억 × 2 = 10억 건
- 쓰기 QPS: 10억 / 86400초 ≈ 11,500 QPS
- 피크 쓰기 QPS: 11,500 × 3 ≈ 35,000 QPS
- 읽기 QPS: (5억 × 50) / 86400 ≈ 289,000 QPS
- 피크 읽기 QPS: 289,000 × 3 ≈ 867,000 QPS
- 일일 저장 (이미지): 10억 × 0.7 × 500KB = 350TB
- 일일 저장 (동영상): 10억 × 0.3 × 3MB = 900TB
- 총 일일 저장: 1,250TB (24시간 후 삭제됨)
- 피크 업로드 대역폭: 35,000 × 평균크기(1.4MB) ≈ 49GB/s
- 피크 다운로드 대역폭: 867,000 × 1.4MB ≈ 1.2TB/s
- 캐시 용량 (인기 20%): 250TB

### 1단계: 문제 이해 및 설계 범위 확정

질문 예시
- 스토리는 정확히 24시간 후 삭제되나요?
- 사용자는 스토리를 몇 개까지 업로드할 수 있나요?
- 실시간 조회수를 표시해야 하나요?
- 스토리에 댓글/좋아요 기능이 필요한가요?
- 스토리를 친구만 볼 수 있게 설정할 수 있나요?
- 스토리 다운로드 기능이 필요한가요?

설계 범위 (가정)
- 스토리는 업로드 후 정확히 24시간 뒤 삭제
- 이미지/동영상 모두 지원
- 실시간 조회 수 표시
- 친구 공개/전체 공개 설정 가능
- 댓글/좋아요 기능 포함

### 2단계: 개략적 설계안

핵심 컴포넌트
1. 업로드 서비스: 이미지/동영상 업로드 처리
2. CDN: 전 세계 빠른 전송
3. 메타데이터 DB: 스토리 정보, 조회 수, 작성자
4. 타임라인 서비스: 친구 스토리 목록 생성
5. 삭제 서비스: 24시간 후 자동 삭제
6. 알림 서비스: 새 스토리 업로드 알림

API 설계
- POST /api/v1/stories (업로드)
- GET /api/v1/stories/timeline (타임라인 조회)
- GET /api/v1/stories/{storyId} (스토리 조회)
- POST /api/v1/stories/{storyId}/views (조회 수 증가)
- DELETE /api/v1/stories/{storyId} (삭제)

데이터 모델
- stories 테이블: story_id, user_id, media_url, created_at, expires_at, privacy
- views 테이블: story_id, viewer_id, viewed_at
- timeline_cache: user_id, friend_stories[]

### 3단계: 상세 설계

집중할 포인트
1. 24시간 자동 삭제 메커니즘
   - TTL(Time To Live) 기반 자동 삭제
   - Redis sorted set으로 만료 시간 관리
   - 배치 작업으로 스토리 정리

2. 읽기 성능 최적화 (867K QPS!)
   - CDN으로 미디어 파일 캐싱
   - Redis로 타임라인 캐싱
   - 조회 수는 비동기 업데이트

3. 쓰기 성능 최적화
   - 이미지 업로드 후 비동기 인코딩
   - 여러 해상도 생성 (썸네일, 중간, 원본)
   - 친구에게 Push 알림 (Fan-out)

4. 저장 공간 최적화
   - 24시간 후 확실한 삭제로 비용 절감
   - 이미지 압축 (WebP 포맷)
   - 동영상 트랜스코딩

### 4단계: 마무리

개선 사항
- 지역별 CDN 서버로 지연시간 감소
- ML로 부적절한 콘텐츠 자동 필터링
- 스토리 하이라이트 (24시간 후에도 보관)
- 스토리 다운로드 기능
- 조회자 목록 표시

병목 지점
- 피크 시간대 읽기 QPS (867K): CDN + Redis 캐시 필수
- 피크 업로드 대역폭 (49GB/s): 분산 업로드 서버
- 24시간 후 대량 삭제: 배치 작업 최적화

---

## 예제 2: YouTube 조회 수 카운터 (난이도: 중상)

문제
"YouTube처럼 수십억 건의 조회 수를 실시간으로 카운팅하는 시스템을 설계하세요."

### STEP 0: 규모 추정 (실습 필수!)

주어진 조건
- 일간 능동 사용자(DAU): 20억 명
- 사용자당 하루 평균 동영상 시청: 10개
- 동영상 총 개수: 10억 개
- 조회 수 업데이트 지연 허용: 1분 이내
- 조회 수 표시 정확도: 큰 수는 근사값 허용 (예: 1.2M)

계산해야 할 항목
[ ] 일간 총 조회 수
[ ] 조회 수 업데이트 QPS (평균, 피크)
[ ] 조회 수 읽기 QPS (평균, 피크)
[ ] 메모리 캐시 용량 (인기 동영상 20%)
[ ] DB 쓰기 부하

예상 결과 (스스로 계산 후 확인)
- 일간 조회 수: 20억 × 10 = 200억 건
- 쓰기 QPS: 200억 / 86400 ≈ 231,000 QPS
- 피크 쓰기 QPS: 231,000 × 3 ≈ 693,000 QPS
- 읽기 QPS (페이지 로드 시): 쓰기의 10배 = 2,310,000 QPS
- 피크 읽기 QPS: 6,930,000 QPS
- 캐시 메모리: 10억 개 × 20% × 100바이트 = 20GB

### 1단계: 문제 이해 및 설계 범위 확정

질문 예시
- 조회 수가 정확해야 하나요, 아니면 근사값도 괜찮나요?
- 실시간 업데이트가 필요한가요, 아니면 약간의 지연이 허용되나요?
- 중복 조회를 어떻게 처리하나요? (같은 사용자가 여러 번 새로고침)
- 봇 트래픽을 필터링해야 하나요?
- 조회 수가 매우 큰 인기 동영상의 경우 특별한 처리가 필요한가요?
- 좋아요/싫어요 수도 같이 카운팅하나요?

설계 범위 (가정)
- 조회 수는 근사값 허용 (예: 1,234,567 → 1.2M)
- 1분 이내 업데이트 허용
- 같은 사용자의 중복 조회는 30초 내 1회만 카운트
- 봇 필터링 필요
- 읽기 >> 쓰기 (읽기가 10배 많음)

### 2단계: 개략적 설계안

핵심 컴포넌트
1. 조회 이벤트 수집기: 사용자의 조회 이벤트 수신
2. 메시지 큐 (Kafka): 이벤트 버퍼링
3. 집계 서비스: 조회 수 집계 (배치 처리)
4. 캐시 (Redis): 인기 동영상 조회 수
5. 데이터베이스: 모든 동영상 조회 수 저장
6. API 서버: 조회 수 조회

API 설계
- POST /api/v1/videos/{videoId}/view (조회 이벤트)
- GET /api/v1/videos/{videoId}/count (조회 수 조회)

데이터 모델
- video_views 테이블: video_id, view_count, updated_at
- view_events 큐: video_id, user_id, timestamp, ip_address

### 3단계: 상세 설계

집중할 포인트
1. 쓰기 성능 최적화 (693K 피크 QPS!)
   - Kafka로 이벤트 버퍼링
   - 1분마다 배치로 집계 후 DB 업데이트
   - Redis에서 카운터 증가 (INCR)
   - 비동기 DB 동기화

2. 읽기 성능 최적화 (6.9M 피크 QPS!)
   - Redis에 모든 동영상 조회 수 캐싱
   - CDN에 조회 수 API 응답 캐싱 (1분 TTL)
   - 큰 수는 반올림 (1,234,567 → 1.2M)

3. 정확도 vs 성능 Trade-off
   - 실시간 정확도 포기 → 1분 지연 허용
   - 중복 조회 방지: Redis에 user_id:video_id 30초 TTL
   - 봇 필터링: User-Agent, IP 기반

4. 확장성
   - Kafka 파티션 분할 (video_id 기준)
   - 집계 서비스 수평 확장
   - Redis 클러스터 샤딩

### 4단계: 마무리

개선 사항
- 실시간 조회 수 스트림 (WebSocket)
- 조회 수 증가 애니메이션
- 시간대별 조회 수 추이 그래프
- 지역별 조회 수 분석
- 조회 수 급상승 동영상 탐지

병목 지점
- 읽기 QPS (6.9M): Redis 클러스터 + CDN 필수
- 쓰기 QPS (693K): Kafka 버퍼링 + 배치 처리
- Redis 메모리: 인기 동영상만 캐싱 (전체 캐싱 불가능)

---

## 예제 3: Twitter 트렌드 토픽 (난이도: 상)

문제
"Twitter의 실시간 트렌드 토픽(Trending Topics)을 계산하는 시스템을 설계하세요."

### STEP 0: 규모 추정 (실습 필수!)

주어진 조건
- 일간 능동 사용자(DAU): 3억 명
- 사용자당 하루 평균 트윗: 5개
- 트윗당 평균 단어 수: 20개
- 해시태그 포함 비율: 30%
- 트렌드 업데이트 주기: 5분마다
- 트렌드 토픽 개수: 상위 50개

계산해야 할 항목
[ ] 일간 총 트윗 수
[ ] 일간 총 해시태그 수
[ ] 해시태그 집계 QPS
[ ] 5분간 처리해야 할 트윗 수
[ ] 트렌드 계산 시 메모리 사용량

예상 결과 (스스로 계산 후 확인)
- 일간 트윗: 3억 × 5 = 15억 건
- 트윗 QPS: 15억 / 86400 ≈ 17,400 QPS
- 피크 트윗 QPS: 17,400 × 3 ≈ 52,000 QPS
- 해시태그 포함 트윗: 15억 × 0.3 = 4.5억 건
- 5분간 트윗: 52,000 × 300초 = 1,560만 건
- 5분간 해시태그: 1,560만 × 0.3 = 468만 개
- 해시태그 종류: 약 100만 개 (유니크)
- 메모리 사용: 100만 × (해시태그 50바이트 + 카운트 8바이트) ≈ 58MB

### 1단계: 문제 이해 및 설계 범위 확정

질문 예시
- 글로벌 트렌드인가요, 지역별 트렌드인가요?
- 실시간 업데이트가 필요한가요, 아니면 5-10분 지연이 허용되나요?
- 트렌드 계산 알고리즘은 단순 빈도수인가요, 아니면 급상승 트렌드인가요?
- 스팸/봇 트윗을 필터링해야 하나요?
- 개인화된 트렌드를 제공하나요?
- 과거 트렌드 데이터를 저장해야 하나요?

설계 범위 (가정)
- 지역별 트렌드 (국가/도시별)
- 5분마다 업데이트
- 급상승 트렌드 우선 (단순 빈도수 아님)
- 봇 필터링 필요
- 개인화 없음 (모두에게 동일)
- 과거 24시간 트렌드 히스토리 저장

### 2단계: 개략적 설계안

핵심 컴포넌트
1. 트윗 스트림 수집: 실시간 트윗 수신
2. 해시태그 추출기: 트윗에서 해시태그 파싱
3. 집계 서비스: 5분 윈도우로 해시태그 카운팅
4. 트렌드 계산 엔진: 급상승 트렌드 알고리즘
5. 캐시 (Redis): 현재 트렌드 저장
6. 데이터베이스: 과거 트렌드 히스토리

API 설계
- GET /api/v1/trends/global (글로벌 트렌드)
- GET /api/v1/trends/{location} (지역별 트렌드)
- GET /api/v1/trends/history (과거 트렌드)

데이터 모델
- trending_topics: hashtag, count, score, location, timestamp
- hashtag_history: hashtag, hourly_counts[], date

### 3단계: 상세 설계

집중할 포인트
1. 실시간 집계 (52K 피크 QPS)
   - Kafka Streams로 실시간 처리
   - 5분 슬라이딩 윈도우
   - 지역별 파티션 분할

2. 트렌드 스코어 계산
   - 단순 빈도수 아님!
   - 급상승 여부 = (현재 5분 카운트) / (과거 1시간 평균)
   - 시간 가중치 (최근 트윗에 높은 가중치)
   - 공식: score = (current_count / historical_avg) × time_weight

3. 확장성
   - Kafka 파티션으로 지역별 분산 처리
   - 집계 서비스 수평 확장
   - Redis에 상위 50개만 캐싱

4. 봇 필터링
   - 동일 계정의 동일 해시태그 반복 제거
   - 의심 계정 패턴 탐지
   - Rate Limiting

### 4단계: 마무리

개선 사항
- 개인화 트렌드 (사용자 관심사 기반)
- 트렌드 변화 알림
- 특정 주제 트렌드 (스포츠, 정치 등)
- 트렌드 예측 (ML 기반)
- 트렌드 시각화 (차트, 워드클라우드)

병목 지점
- 피크 시간대 트윗 처리 (52K QPS): Kafka 버퍼링
- 5분마다 100만 개 해시태그 집계: 분산 처리 필수
- 트렌드 스코어 계산 복잡도: 캐시 활용

---

## 실습 진행 가이드

### 타임라인 (총 50분)

1. 규모 추정 (10분)
   - 계산기 사용 금지, 어림셈 연습
   - 2의 제곱수 암기 활용 (2^10=1024, 2^20=1M)
   - 결과를 화이트보드에 크게 표시

2. 1단계: 문제 정의 (5분)
   - 최소 5개 질문하기
   - 가정 사항 명확히 하기

3. 2단계: 개략적 설계 (15분)
   - 다이어그램 그리기
   - API 정의
   - 데이터 모델 설계

4. 3단계: 상세 설계 (15분)
   - 병목 지점 3개 찾기
   - 각 병목에 대한 해결책 제시
   - Trade-off 설명

5. 4단계: 마무리 (5분)
   - 개선 사항 3가지
   - 모니터링 포인트
   - 장애 대응

### 규모 추정 꿀팁

필수 암기 숫자
- 1일 = 86,400초 ≈ 100,000초 (어림셈)
- 1달 = 2,592,000초 ≈ 2.5M초
- 1년 = 31,536,000초 ≈ 30M초
- 2^10 = 1024 ≈ 1K
- 2^20 = 1,048,576 ≈ 1M
- 2^30 = 1,073,741,824 ≈ 1B

QPS 계산 공식
- 평균 QPS = 일일 요청 수 / 100,000
- 피크 QPS = 평균 QPS × 2~3

저장 용량 어림셈
- KB → MB: ÷ 1,000
- MB → GB: ÷ 1,000
- GB → TB: ÷ 1,000

### 체크리스트

규모 추정
- [ ] DAU/MAU 계산
- [ ] 읽기 QPS 계산
- [ ] 쓰기 QPS 계산
- [ ] 피크 QPS 계산 (평균의 2-3배)
- [ ] 저장 용량 계산 (1일, 1년, 5년)
- [ ] 대역폭 계산
- [ ] 캐시 메모리 계산

4단계 접근법
- [ ] 명확한 요구사항 도출
- [ ] API 설계
- [ ] 데이터 모델 정의
- [ ] 개략적 아키텍처 다이어그램
- [ ] 병목 지점 식별
- [ ] 확장성 고려
- [ ] Trade-off 설명
- [ ] 개선 사항 제시

---

## 스터디 진행 예시

팀 구성
- 2-4명이 적정
- 1명은 면접관 역할
- 나머지는 지원자 역할
- 30분마다 역할 교대

진행 방식
1. 면접관이 문제 제시
2. 지원자는 규모 추정부터 시작 (반드시!)
3. 4단계 접근법 순서대로 진행
4. 면접관은 중간에 질문/피드백
5. 종료 후 상호 피드백 (10분)

피드백 포인트
- 규모 추정 정확도
- 질문의 적절성
- 설계의 합리성
- Trade-off 이해도
- 시간 배분
- 커뮤니케이션

---
