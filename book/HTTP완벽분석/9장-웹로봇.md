# 9장 웹로봇

## 웹로봇 개요

웹로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램이다. 웹사이트를 탐색하고, 콘텐츠를 수집하고, 하이퍼링크를 따라가며, 발견한 데이터를 처리한다.

### 웹로봇의 특징
- 자동으로 HTTP 요청을 만들어 웹 콘텐츠 수집
- 사람의 개입 없이 연속적인 작업 수행
- 다양한 목적으로 웹을 탐색하고 처리

## 웹로봇의 종류

### 1. 주식 그래프 로봇
- 매분마다 주식 시장 데이터 요청
- 실시간 시장 정보 수집 및 분석
- 자동화된 거래 시스템과 연동

### 2. 검색엔진 로봇 (스파이더)
- 문서를 수집하여 검색 데이터베이스 구축
- 전문 색인(full-text index) 생성
- 웹 페이지 인기도 측정 (링크 분석)

### 3. 웹 크롤러
- 루트 집합(root set)에서 시작하여 재귀적으로 웹 탐색
- 발견된 모든 링크를 따라가며 문서 수집
- 웹의 구조를 체계적으로 탐색

### 4. 링크 체커
- 웹사이트의 깨진 링크 검사
- 링크 유효성 검증
- 사이트 품질 관리 도구

---

## 크롤러와 크롤링

### 크롤러 동작 원리

**1. 루트 집합에서 시작**
- 초기 URL 목록(seed URLs)으로 크롤링 시작
- 중요하거나 인기 있는 사이트 우선 선택

**2. 링크 추출 및 정규화**
- HTML 파싱하여 링크 추출
- 상대 경로를 절대 경로로 변환
- URL 정규화 (대소문자, 이스케이프 문자 등)

**3. 순환 크롤링 방지**
- 이미 방문한 URL 추적
- URL 정규화로 중복 제거
- 콘텐츠 핑거프린팅으로 중복 콘텐츠 감지

### 크롤링 전략

**너비 우선 크롤링(Breadth-First Crawling)**
- 현재 레벨의 모든 페이지를 먼저 방문
- 사이트 전체를 균등하게 탐색
- 중요한 페이지를 먼저 발견할 가능성 높음

**깊이 우선 크롤링(Depth-First Crawling)**
- 한 경로를 끝까지 따라간 후 다음 경로 탐색
- 특정 사이트를 집중적으로 탐색
- 메모리 사용량이 적음

### 크롤링 시 고려사항

**1. 동적 가상 웹 공간 문제**
- 무한히 생성되는 동적 페이지 회피
- URL 패턴 분석으로 함정 감지
- 최대 깊이 제한 설정

**2. 중복 페이지 처리**
- URL 정규화
- 콘텐츠 해시를 통한 중복 감지
- 이미 방문한 URL 데이터베이스 유지

**3. 서버 부하 관리**
- 요청 간격 조절(throttling)
- 동시 연결 수 제한
- robots.txt 준수

---

## 로봇의 HTTP

### 요청 헤더 식별

**User-Agent 헤더**
- 로봇의 이름과 버전 명시
- 연락처 정보 포함 권장
```http
User-Agent: Googlebot/2.1 (+http://www.google.com/bot.html)
```

**From 헤더**
- 로봇 관리자의 이메일 주소
- 문제 발생 시 연락처 제공
```http
From: webmaster@example.com
```

**Accept 헤더**
- 로봇이 처리 가능한 콘텐츠 타입 명시
- 불필요한 콘텐츠 다운로드 방지
```http
Accept: text/html, application/xml
```

### 가상 호스팅 처리

**Host 헤더 필수**
- 가상 호스팅 서버에서 올바른 콘텐츠 제공
- HTTP/1.1 준수
```http
Host: www.example.com
```

### 조건부 요청

**If-Modified-Since 헤더**
- 마지막 방문 이후 변경된 콘텐츠만 다운로드
- 대역폭 절약 및 서버 부하 감소
```http
If-Modified-Since: Mon, 01 Jan 2024 00:00:00 GMT
```

---

## 로봇 차단 표준 (Robots Exclusion Standard)

### robots.txt 파일

웹사이트 루트에 위치하는 로봇 접근 제어 파일로, 어떤 로봇이 사이트의 어떤 부분에 접근할 수 있는지 명시한다.

**기본 문법**
```
User-agent: *
Disallow: /private/
Disallow: /tmp/
Allow: /public/

User-agent: Googlebot
Disallow: /no-google/

User-agent: BadBot
Disallow: /
```

### robots.txt 파일 처리

**1. 파일 가져오기**
- 사이트 방문 전 `http://site.com/robots.txt` 요청
- 404 응답 시 제한 없음으로 간주
- 401/403 응답 시 모든 접근 금지로 간주

**2. 규칙 적용**
- User-agent별 규칙 확인
- 가장 구체적인 규칙 우선 적용
- Allow/Disallow 패턴 매칭

**3. 캐싱**
- robots.txt 파일 캐싱으로 반복 요청 방지
- 주기적으로 갱신 (일반적으로 하루 한 번)

### 확장 기능

**Crawl-delay 지시자**
```
User-agent: *
Crawl-delay: 10
```
- 요청 간 최소 대기 시간(초) 지정

**Sitemap 지시자**
```
Sitemap: https://www.example.com/sitemap.xml
```
- 사이트맵 위치 명시

---

## HTML 로봇 제어 태그

### META 로봇 태그

페이지 단위로 로봇 동작을 제어하는 HTML 메타 태그

**기본 사용법**
```html
<meta name="robots" content="noindex, nofollow">
```

**content 값 옵션**
- `index/noindex`: 페이지 색인 허용/금지
- `follow/nofollow`: 링크 추적 허용/금지
- `archive/noarchive`: 페이지 캐싱 허용/금지
- `snippet/nosnippet`: 검색 결과 스니펫 표시 허용/금지
- `all`: 모든 작업 허용 (기본값)
- `none`: 모든 작업 금지

**특정 로봇 대상 지정**
```html
<meta name="googlebot" content="noindex">
<meta name="bingbot" content="nofollow">
```

### 링크 단위 제어

**rel="nofollow" 속성**
```html
<a href="http://example.com" rel="nofollow">외부 링크</a>
```
- 특정 링크를 따라가지 않도록 지시
- 페이지랭크 전달 차단

---

## 검색엔진 로봇

### 검색엔진 아키텍처

**1. 크롤링 단계**
- 웹 페이지 발견 및 수집
- URL 프론티어(frontier) 관리
- 우선순위 기반 크롤링

**2. 인덱싱 단계**
- 전문 색인(full-text index) 생성
- 역색인(inverted index) 구축
- 메타데이터 추출 및 저장

**3. 랭킹 단계**
- 페이지랭크 알고리즘
- 관련성 점수 계산
- 사용자 의도 분석

### 검색엔진 최적화(SEO) 고려사항

**크롤러 친화적 구조**
- 명확한 URL 구조
- XML 사이트맵 제공
- 내부 링크 최적화

**콘텐츠 접근성**
- JavaScript 렌더링 콘텐츠 처리
- 모바일 친화적 설계
- 빠른 페이지 로딩 속도

---

## 로봇 에티켓

### 로봇 운영 가이드라인

**1. 신원 표시**
- 명확한 User-Agent 설정
- 연락처 정보 제공
- 로봇 정보 페이지 운영

**2. 서버 부하 최소화**
- 요청 속도 제한 (throttling)
- 동시 연결 수 제한
- 피크 시간 회피

**3. 규칙 준수**
- robots.txt 존중
- META 로봇 태그 준수
- 사이트 이용약관 확인

**4. 콘텐츠 처리**
- 저작권 존중
- 개인정보 보호
- 수집 목적 명시

### 악의적 로봇 대응

**문제가 되는 로봇 행동**
- 과도한 요청으로 서버 과부하
- robots.txt 무시
- 개인정보 수집
- 콘텐츠 도용

**대응 방법**
- IP 차단
- User-Agent 필터링
- CAPTCHA 적용
- Rate limiting
- 허니팟(honeypot) 설치